---
title: "ESM 244 Lab 7"
author: "Meghan Bowen"
date: "2/21/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_packages, message=FALSE}

library(tidyverse)
library(tmap)
library(sf)
library(spatstat)
library(maptools)
library(sp)
library(raster)
library(gstat)
library(knitr)

```

## Part 1. Hawaii raster practice

```{r hawaii_rasters}

hi_par <- raster("PAR_CLIM_M.tif")
hi_sst <- raster("SST_LTM.tif")
hi_chl <- raster("CHL_LTM.tif")

par(mfrow=c(1,3))
plot(hi_par)
plot(hi_sst)
plot(hi_chl)
```

NOTE: for thematic (categorical) raster data, consider using method = "ngm" (ngb? nearest neighbor)

```{r manipulate_sst}
# hi_sst@crs == show projection
# reproject from NAD83 to WGS84

# create a character string with WGS84 information, to use in multiple functions
wgs84 <- "+proj=longlat +datum=WGS84 +ellps=WGS84 +no_defs"

# reproject sea surface temp to WGS84
hi_sst_84 <- projectRaster(hi_sst, 
                           crs = wgs84, 
                           method = "bilinear") #could have typed in all of this, or just save as wgs84


# lower the raster resolution: use "aggregate" (by what factor do you want to aggregate cells)
# maybe ya wanna work with easier data instead of huge freaking rasters!

sst_rs <- aggregate(hi_sst, 
                    fact = 10)
plot(sst_rs)


# crop a raster
# what is current extent of reprojected sst (hi_sst_84@extent)?

#class       : Extent 
#xmin        : -160.4705 
#xmax        : -154.5137 
#ymin        : 18.7309 
#ymax        : 22.44634 

sst_bounds <- as(extent(-156.2, -154.5, 18.7, 20.4), 
                 'SpatialPolygons')
#as - "general wrapper, convert something in here to this type of object"
#extent is manual, chosen by Allison

# since there's no projection associated with sst_bounds, you can assign it to match hi_sst_84
crs(sst_bounds) <- crs(hi_sst_84)

# projections match, let's crop
sst_crop <- crop(hi_sst_84, sst_bounds)
plot(sst_crop)
```

Now let's do some raster calcuations, hoorary

- let's make a nonsensical variable called "tropicality", that is the sum of PAR + SST + 2*CHL(a)
- then map tropicality

```{r tropicality}

# reproject PAR and CHL
hi_par_84 <- projectRaster(hi_par, 
                           crs = wgs84, 
                           method = "bilinear")

hi_chl_84 <- projectRaster(hi_chl,
                           crs = wgs84,
                           method = "bilinear")

# scales are different for each variable
par(mfrow = c(1,3))
plot(hi_sst_84)
plot(hi_par_84)
plot(hi_chl_84)

# new variable
# error: "Raster objects have different extents. Result for their intersection is returned"
# later: how to coerce raster objects to have same # of cols and rows

trop <- hi_par_84 + hi_sst_84 + 2*hi_chl_84
plot(trop)
```

Now let's use tmap, my absolute favorite mapping functionality in R

```{r hawaii_tmap}

islands <- read_sf(dsn = 'islands', layer = "Island_boundaries") %>% 
  dplyr::select(Island) %>%  #select function is also in spatial stats, will not work if spatial stats
  st_simplify(dTolerance = 10) %>% 
  st_transform(crs = 4326)

tmap_mode("plot") #static vs. interactive plotting

sst_map <- tm_shape(hi_sst_84) +
  tm_raster(title = "Mean Sea Surface Temperature") +
  tm_layout(bg.color = "navyblue",
            legend.position = c("left", "bottom"),
            legend.text.color = "white",
            legend.text.size = 0.5) +
  tm_shape(islands) +
  tm_fill("darkgreen")

# tmap_save : create any file that's stored!
# this will get saved in the working directory (esm-244-lab-7)
tmap_save(sst_map, "hawaii_sst_map.tiff", height = 5)

```

### Conditional rasters and masking

Let's say we have a sensitive species and we're trying to find habitat that it might like

- It likes warm water (average SST >= 25.6 degrees)
- Likes solar (PAR) below 54

```{r hawaii_mask}

# change sst extent to match par
extent(hi_sst_84) <- extent(hi_par_84)

# check rasters compatibility - mismatched rows & Columns
compareRaster(hi_sst_84, hi_par_84)

# make a new raster from scratch, so sst and par will match
# these values are from the call for hi_par_84, dimensions and extent
cr <- raster(nrow = 822, 
             ncol = 1229, 
             xmn = -160.4365, 
             xmx = -154.5373, 
             ymn = 18.7309, 
             ymx = 22.44634)

sst_new <- resample(hi_sst_84, hi_par_84, method = "bilinear") 
#could also put in cr instead of hi_par_84, but it should be fine

compareRaster(sst_new, hi_par_84)
plot(sst_new)
plot(hi_par_84)


# crop for Kauai (manual crop courtesy of Allison)
bounds_main <- as(extent(-159.9, -159.2, 21.7, 22.3), 'SpatialPolygons')
crs(bounds_main) <- crs(sst_new)

par_kauai <- crop(hi_par_84, bounds_main)
sst_kauai <- crop(sst_new, bounds_main)

par(mfrow=c(1,2))
plot(par_kauai, main = "par/Solar")
plot(sst_kauai, main = "SST")
```

Now we only want regions where temp >= 25.4, PAR <54

```{r kauai}

par_habitat <- par_kauai
par_habitat[par_habitat >= 54] <- NA

sst_habitat <- sst_kauai
sst_habitat[sst_habitat < 25.4] <- NA

par(mfrow = c(1,2))
plot(par_habitat)
plot(sst_habitat)

```

## Part 2. Point Pattern Analysis

Looking at red tree voles in Humboldt County (observations, so each point is one observation)

```{r load_vole_data}

voles <- read_sf(dsn = 'redtreevoledata', layer = "ds033") %>% 
  dplyr::select(COUNTY) %>% 
  filter(COUNTY == "HUM") %>% 
  st_transform(crs = 4326)

# Also need Humboldt County outline. No .prj file, so no projection for county info

humboldt <- read_sf(dsn = 'redtreevoledata', layer = "california_county_shape_file") %>% 
  filter(NAME == "Humboldt") %>% 
  dplyr::select(NAME)
st_crs(humboldt) <- 4326 #add projection for humboldt county

```

```{r vole_tmap}
# tmap to plot

tm_shape(humboldt) +
  tm_fill() +
  tm_shape(voles) +
  tm_dots(size=0.2)

# ggplot to plot

humvoles <- ggplot()+
  geom_sf(data=humboldt) +
  geom_sf(data=voles)

# ggsave to save ggplot output. defaults to last plot displayed, but can add plot = x line to specify plot
ggsave("humvoles.tiff",
       units = "in",
       width = 4,
       height = 6,
       dpi = 300)
```

Now we want to explor point patterns in a few different ways

1. Quadrat analysis
2. Distance based (neighbor analysis, G-function and K-function)

```{r vole_patterns}

voles_sp <- as(voles, 'Spatial')
voles_ppp <- as(voles_sp, "ppp")

class(voles)
class(voles_sp)
class(voles_ppp)

# need to make a bounding window. Ya need a limit
# put in Humboldt county boundary

humboldt_sp <- as(humboldt, 'Spatial')
humboldt_window <- as(humboldt_sp, "owin") #owin = outer window

voles_pb <- ppp(voles_ppp$x, voles_ppp$y,
                window = humboldt_window)

```

Quadrat test = test for spatial evenness, splitting a region of interest up into different areas, finding intensity of observations in each area, and how does it compare to the null hypothesis that the intensity in each area is the same.

```{r vole_quadrat}

vole_qt <- quadrat.test(voles_pb, 
                        nx = 5, ny = 10) #number of colums in x and y to split up, chosen by Allison

#when checking the actual splits (5 X 10), will have some squares with little/zero observations. R gives you a head's up that the chi^2 may be skewed.
#the way you break up your quadrats will change your outputs in your analysis. tread wisely
#allison recommends using a precedent (from peer review)

# Testing the null hypothesis of spatial evenness (although you'll hear it called a test for complete spatial randomness (CSR))
vole_qt

# Reject: based on these observations, they do not follow a pattern of spatial evenness


plot(voles_pb)
plot(vole_qt, add = T, cex = 0.4)

```

Plotting kernel densities for spatial data

```{r kernel_musterd}

point_density <- density(voles_pb, 
                         sigma = 0.02) #set bandwith. allison pre-selected for us

point_density_wack <- density(voles_pb,
                              sigma = 1)

# output is entirely dependent on bandwith. tread wisely
par(mfrow = c(1,2))
plot(point_density)
plot(point_density_wack)


# make a point density raster

vole_raster <- raster(point_density,
                      crs = wgs84) #assigned earlier in lab

tm_shape(vole_raster) +
  tm_raster()

```

Nearest Neighbor

- G-function: considers the distance of each boservation to its NEAREST neighbor
- K-function: considers how close all neighboring observations are to an event (concentric circles)
- G fxn very fast in R. K fxn is more slow (using a lot of brain power)

```{r g_fxn}

r <- seq(0, 0.15, by = 0.005) #make a sequence of distances over which you calculate g-fxn values

# envelope: wrapper, creates simulations of "what you'd expect"

gfunction <- envelope(voles_pb, #bounding window and # of events information
                      fun = Gest, #in some package already, who knows
                      r = r, #distances
                      nsim = 20) # number of monte carlo simulations


plot(gfunction$obs ~ gfunction$r,
     type = "l",
     col = "black")
lines(gfunction$theo ~ gfunction$r,
      type = "l",
      col="red")

# at around 0.15, about 100% of observations have a nearest neighbor w/in range
# theoretcial CSR in red : a higher number of our observations occur at closer differences compared to model predictions. 
# THEREFORE, our observations are more clustered than CSR (our observations are probably NOT spatially random)

```

```{r k_fxn}

#asking how close ALL neighbors are to EVERY event in the spatial window

r2 <- seq(0, 0.5, by = 0.05) 
# different sequence of distances? why larger from g function? 
# looking at a much bigger space, considering multiple neighbors (instead of just closest neighbor)

lfunction <- envelope(voles_pb,
                      fun = Lest,
                      r = r2,
                      nsim = 20,
                      global = T) #apply to whole study area

# plot

plot(lfunction$obs ~ lfunction$r,
     type = "l",
     col = "blue")
lines(lfunction$theo ~ lfunction$r,
      type = "l",
      col = "red")

# similar results to g-fxn. more observations smaller distances from our data compared to the predicted model. 
# overall, globally, intensities of events are higher at closer distances than compared at a CSR scenario
```

Evidence that YES, these data are more clustered than CSR


Hypothesis test for CSR: Diggle-Cressie-Loosmore-Ford test 

*Why give this less weight than what you see?*

- still fall into similar traps w/ hypothesis testing: if you have a large sample size, can still find significance 

```{r dclf_test}

DCLFTest <- dclf.test(voles_pb,
                      nsim = 30) #save time, running only a few MC

DCLFTest #yes, significantly different from CSR
```

## Part 3. Spatial interporlation by kriging

Kansas rainfall, heavy flow in some parts of the state (AMT in inches)

```{r kansas_rain}

# LAT and LON, but R doesn't have any spatial information right now
ks_rain <- read_csv("KSRain2.csv")


# use SF to get some geometry

ks_sf <- st_as_sf(ks_rain,
                  coords = c("LON","LAT"), #don't mess up the order!
                  crs = 4326)


# get the Kansas counties info into R. already spatial data, use read_sf. no .prj, put in a projection

ks_counties <- read_sf(dsn = 'KSCounties', layer = "ks_counties_shapefile")
st_crs(ks_counties) <- 4326

```

```{r kansas_tmap}

tm_shape(ks_counties) +
  tm_fill() +
  tm_shape(ks_sf) +
  tm_dots("AMT",      #can call an attribute here
          size = 0.5) 

```

If kriging will work as expected, where will there be the highest error?

- NW area: not a lot of observations up there, so there will likely be high errors in predictions

```{r kansas_kriging}

# Step 0. kriging wants "spatial points" [sp] objects, need to convert

ks_sp <- as_Spatial(ks_sf)
class(ks_sp)


# Step 1. Make a spatial grid to interpolate values over
## predetermined by Allison, what makes sense for grid area (lat & long for kansas)
lat <- seq(37, 40,
           length.out = 200) 
long <- seq(-94.6, -102,
            length.out = 200)


# Step 2. Make R identify these two vectors as a grid
grid <- expand.grid(lon = long, lat = lat)


# Step 3a. convert grid to sf object
grid_sf <- st_as_sf(grid,
                    coords = c("lon", "lat"),
                    crs = 4326)
# Step 3b. Now convert  to spatial points object. It's what the spatial stats package understands
grid_sp <- as_Spatial(grid_sf)

class(grid_sp)


# Step 4. Make a variogram and find the variogram model
ks_vgm <- variogram(AMT ~ 1, ks_sp) #why ~ 1? this indicates ORDINARY kriging (we don't know the statinoary mean or model trend)

plot(ks_vgm)

# Step 5. Fit the variogram model
ks_vgm_fit <- fit.variogram(ks_vgm,
                            model = vgm(nugget = 0.2,
                                        psill = 1.2,
                                        range = 200,
                                        model = "Sph")) #need to provide reasonable initial estimates for nugget, p(artial)sill, range. it's an iterative process to convergence. 
## can also choose other than "Spherical" model: Gaussian or Exponential. Allison says sum of squares for squares is lowest for spherical, so we used that one.

plot(ks_vgm, ks_vgm_fit)


# Step 6. Now we can do the kriging
ks_krige <- gstat::krige(AMT ~ 1,
                         ks_sp, #actual data
                         grid_sp, #grid
                         ks_vgm_fit) #variogram (weights for surrounding observations)

# easier to work with outputs in a df
ks_krige_df <- as.data.frame(ks_krige)
ks_krige_2 <- ks_krige_df %>% 
  rename(lon = coords.x1, lat = coords.x2, predicted = var1.pred, err = var1.var)

```

```{r plot_krige}

rain_predicted <- st_as_sf(ks_krige_2,
                           coords = c("lon", "lat"),
                           crs = 4326)

# get base information for kansas outline

ks <- read_sf(dsn = "states", layer = "cb_2017_us_state_20m") %>% 
  dplyr::select(NAME) %>% 
  filter(NAME == "Kansas") %>% 
  st_transform(crs = 4326)

# crop rainfall data

rain_cropped <- st_intersection(rain_predicted, ks)
plot(rain_cropped)
```